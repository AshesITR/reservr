---
documentclass: jss
author:
  - name: Alexander Rosenstock
    affiliation: |
      | Heinrich-Heine-Universität Düsseldorf \AND
      | ARAG SE
    address: |
      | Mathematisches Institut
      | Heinrich-Heine-Universität Düsseldorf
      | Universitätsstraße 1, 40225 Düsseldorf, Germany
    email: \email{alexander.rosenstock@hhu.de}
title:
  formatted: "Fitting Distributions and Neural Networks to Censored and Truncated Data: The \\proglang{R} Package \\pkg{reservr}"
  plain:     "Fitting Distributions and Neural Networks to Censored and Truncated Data: The R Package reservr"
  short:     "\\pkg{reservr}: Fitting Distributions and Neural Networks to Censored and Truncated Data"
abstract: >
  While the problem of estimating distribution parameters from truncated data with fixed truncation points has readily
  available implementations in \proglang{R}, this is not the case for randomly truncated data, i.e. observations where
  the point of truncation varies by observation.
  In practice, it is often of interest to also consider the influence of other parameters (predictors) on the
  distribution of an outcome variable.
  We present the features and implementation of the \proglang{R} package \pkg{reservr} for a large class of
  distributions.
  The package provides a flexible interface to specify distribution families, provides algorithms to perform parameter
  estimation for censored and randomly truncated data based on (conditional) maximum likelihood, and exposes an
  interface to the \proglang{R} package \pkg{tensorflow} for training flexible neural network models on censored and
  randomly truncated outcomes based on predictors.
  Additional utilities for application in a general insurance context, as well as the usual sampling, density,
  probability and quantile functions are provided.
keywords:
  formatted: [distribution fitting, truncation, random truncation, censoring, "\\proglang{R} packages"]
  plain:     [distribution fitting, truncation, random truncation, censoring, R packages]
preamble: >
  \usepackage{amsmath}
  \usepackage{amssymb}
output: rticles::jss_article
bibliography: bibliography.bib
---

```{r, setup, include=FALSE}
library(reservr)
options(prompt = 'R> ', continue = '+ ')
```

# Introduction

Most statistical analyses are concerned with modelling and estimation of the distribution of some measured variable of interest, the outcome, possibly conditional on the value of one or several endogenous variables.
In the absence of endogenous variables, this process is usually called distribution fitting, and in the presence of endogenous variables it is called regression.
Classical regression, such as via generalized linear models, is usually concerned with the influence of endogenous variables on the mean of the outcome, i.e., $\mathbb{E}(Y|X) = f(X)$ for some random variable $Y$ and vector of associated endogenous variables $X$ paired with a homoskedascity assumption linking the other parameters of the outcome distribution to its mean.
Some models also allow specification of additional parameters of the distribution of the outcome variable, such as GAMLSS, which also allows for the scale parameter to depend on endogeneous variables.
More recently, deep distributional regression allows for flexible specification of individual outcome distribution parameters.
An example implementation of the latter can be found in \citet{deepregression}.

Usual statistical methods require complete data, that is full information on all observations of interest (X, Y).
In addition to this usual scenario we study two observation schemes that do not provide all observations.
Data with _interval censoring_ applied to the outcome $Y$ only consists of lower and upper bounds for $Y$ instead of the actual value.
Truncated data misses observations for which the outcome $Y$ falls out of a certain lower and upper truncation bound.
We consider the case of _random truncation_, where these truncation bounds are also random variables that vary for each observation.

[//]: # (TODO continue here)

Interval censored observations can be described as follows: let $(Y, M, V)$ denote a random vector, where $Y$ is the variable of interest that is supposed to have a density $f_\theta$ with respect to some dominating sigma-finite measuer $\mu$.
The pair $(M, V)$ shall satisfy $M \le Y \le V$, with $M$ possibly equal to $-\infty$ and $V$ possibly equal to $+\infty$.
Further, assume $Y$ to be conditionally independent of $(M, V)$ given $M \le Y \le V$ and $(M, V)$ to have density $f_{(M, V)}$ with respect to some dominating sigma-finite measure $\nu$.
A complete sample $(Y, M, V)$ is therefore drawn from the $(\mu \otimes \nu)$-density

\begin{align}
  f_{(Y, M, V) | M \le Y \le V}(y, m, v) & = f_{Y | M \le Y \le V}(y) f_{(M, V) | M \le Y \le V}(m, v) \nonumber\\
& = \frac{f_\theta(y)}{\mathrm{Pr}(M \le Y \le V)} f_{(M, V) | M \le Y \le V}(m, v) \mathbf{1}(m \le y \le v)
\end{align}


The random truncation problem can be formulated as follows: let $(Y, L, U)$ denote a random vector, where $Y$ is the variable of interest that is supposed to have a density $f_\theta$ with respect to some dominating sigma-finite measure $\mu$.
The pair $(L, U)$ is assumed to be independent of $Y$ and shall satisfy $L \le U$, with $L$ possibly equal to $-\infty$ and $U$ possibly equal to $+\infty$.
Further, $(L, U)$ shall have density $f_{(L, U)}$ with respect to some dominating sigma-finite measure $\nu$.
A sample of _randomly truncated observations_ from $(Y, L, U)$ consists of independent observations $(y_i, l_i, u_i)$ that we only happen to see if $l_i \le y_i \le u_i$.
As a consequence, any observed value can be regarded as being drawn from the $(\mu \otimes \nu)$-density

\begin{align}
  f_{(Y, L, U) | L \le Y \le U}(y, l, u) = \frac{f_{(L, U)}(l, u) f_\theta(y)}{\mathrm{Pr}(L \le Y \le U)} \mathbf{1}(l \le y \le u) \label{eq:trunc-dens}
\end{align}

Subsequently, we write $(Y^{(t)}, L^{(t)}, U^{(t)})$ for a random vector following the above density, i.e.,

\[
  f_{(Y^{(t)}, L^{(t)}, U^{(t)})}(y, l, u) = f_{(Y, L, U) | L \le Y \le U}(y, l, u)
\]

Estimating $\theta$ based on maximum likelihood requires specifying a distribution for $(L, U)$ (which can be regarded as a nuisance parameter) and calculating the denominator in \eqref{eq:trunc-dens}.
This (major) nuisance can be avoided by instead considering conditional maximum likelihood, which is known to produce consistent estimators as well.
In our case, we rely on considering the density of $Y^{(t)}$ conditional on the value of $(L^{(t)}, U^{(t)}) = (l, u)$, which is given by

\begin{align*}
  f_{Y^{(t)} | L^{(t)} = l, U^{(t)} = u}(y) & = \frac{f_{(Y^{(t)}, L^{(t)}, U^{(t)})}(y, l, u)}{f_{(L^{(t)}, U^{(t)})}(l, u)} \\
    & = \frac{f_{(Y, L, U) | L \le Y \le U}(y, l, u)}{\int_{[l, u]} f_{(Y, L, U) | L \le Y \le U}(z, l, u) \,\mathrm{d}z} = \frac{f_\theta(y)}{\int_{[l, u]} f_\theta(z) \,\mathrm{d}z}
\end{align*}

Combining interval censoring and random truncation yields the conditional likelihood

\begin{align}
  \ell(\theta; M^{(t)} = m, V^{(t)} = v, L^{(t)} = l, U^{(t)} = u) & = \log F_\theta([m, v]) - \log F_\theta([l, u])
\end{align}

A practical example of random truncation and interval censorship arises in the presence of inaccurate accident date specifications when reporting claims in general insurance.
The truncation bounds $(L, U)$ in this case will be equal to $(0, \tau - t_0)$ where $t_0$ is the actual accident time and $\tau$ is the calendar time.
Censoring bounds could, for example, be $(M, V) = (t_1 - \lceil t_0 \rceil, t_1 - \lfloor t_0 \rfloor)$ representing an inaccurate observation of $t_0$ at reporting time $t_1$.
Technically, $U$ is not (fully) observable in this situation, but can be approximated by $\tau - \lfloor t_0 \rfloor$.

For later purposes, it is helpful to attach a weight $w_i$ to each observation $(m_i, v_i, l_i, u_i)$ (one might think of $w_i = 1$ for the moment).
Denote the resulting sample by $\mathfrak{I} = \{(m_i, v_i, l_i, u_i, w_i)\}$ where the case $m_i = v_i$ denotes a non-censored observation and we have $l_i \le m_i \le v_i \le u_i$ for all observations.
We aim at maximizing

\begin{align}
  \ell(\theta | \mathfrak{I}) & = \sum_{(m, v, l, u) \in \mathfrak{I}} w \cdot \begin{cases}
    \log f_\theta(m) - \log F_\theta ([l, u]) & m = v \\
    \log F_\theta([m, v]) - \log F_\theta([l, u]) & m < v
  \end{cases} \label{eq:cml-likelihood}
\end{align}

## Related packages

For the less general cases of censoring without truncation and truncation without censoring, as well as for estimation of distribution parameters in the absence of any censoring or truncation mechanism, there are a number of \proglang{R} packages that can fit distributions, some of them also supporting weights.
Among these are \pkg{MASS} \citep{MASS}, \pkg{fitdistrplus} \citep{fitdistrplus}, \pkg{evmix} \citep{evmix}, \pkg{ExtDist} \citep{ExtDist}, \pkg{flexsurv} \citep{flexsurv}.
Another R6-based interface is provided by \pkg{ROOPSD} \citep{ROOPSD}.

The remaining parts of this paper are structured as follows: Section&nbsp;\ref{pkg-overview} details the core functionality of the corresponding \proglang{R} package \pkg{reservr}.
It is split into definition of samples $\mathfrak{I}$ (Section&nbsp;\ref{trunc-obs}), definition of distribution families (Section&nbsp;\ref{distributions}), estimation of distribution parameters (Section&nbsp;\ref{fit-dist}) and distributional regression using \pkg{tensorflow} (Section&nbsp;\ref{tensorflow}).
A conclusion is given in Section&nbsp;\ref{conclusion}.

# Usage of \pkg{reservr} {short-title="Usage of reservr" #pkg-overview}

The package serves two main goals: fitting distributions to randomly truncated interval censored data and performing (deep) distributional regression with randomly truncated interval censored data.
Four main components are integrated with each other to facilitate the analysis goals

1. Methods for defining a randomly truncated interval censored sample $\mathfrak{I}$.
2. Methods for specifying distribution families $\mathcal{F}$ to be fitted.
3. Methods for estimating distribution parameters given a sample $\mathfrak{I}$ and a family $\mathcal{F}_\theta$.
4. Methods for regression of distribution parameters given a sample $\mathfrak{I}$ with associated covariates $X$, a family $\mathcal{F}_\theta$ and a general \pkg{tensorflow} network $g$ that processes $X$ to estimate $Y | X = F_{g(x)}$.

Each of these components is described one by one in the following sections.

## Working with samples {#trunc-obs}

A sample $\mathfrak{I} = \{(m, v, l, u, w)_i\}$ is represented as a tibble (from package \pkg{tibble}).
The core function to create this tibble is `trunc_obs()`.
A tibble created by `trunc_obs()` consists of five columns:

 * `x`: If observed, the exact value of the random variable. Otherwise `NA`.
 * `xmin`: Lower interval censoring bound for the observation. If the observation is not censored, `xmin` is equal to `x`.
 * `xmax`: Upper interval censoring bound for the observation. If the observation is not censored, `xmax` is equal to `x`.
 * `tmin`: Lower truncation bound. Only observations with $x \ge t_{\text{min}}$ are observed. Can be $-\infty$ to indicate no lower truncation.
 * `tmax`: Upper truncation bound. Only observations with $x \le t_{\text{max}}$ are obserbed. Can be $\infty$ to indicate no upper truncation.
 * `w`: The weight associated with the observation. Defaults to $1$.

The following code defines a sample of size 1 without truncation and censoring, with the realized value of $1.3$.

```{r}
trunc_obs(1.3)
```

Simulating randomly truncated and interval censored data from a standard normal distribution with $80\%$ of the observations randomly interval censored and random uniform truncation $L \sim U[-2, 0]$ and $U \sim U[0, 2]$ can be simulated as follows

```{r}
set.seed(123)
N <- 1000L
x <- rnorm(N)
is_censored <- rbinom(N, size = 1L, prob = 0.8) == 1L
x_lower <- x
x_lower[is_censored] <- x[is_censored] -
  runif(sum(is_censored), min = 0, max = 0.5)
x_upper <- x
x_upper[is_censored] <- x[is_censored] +
  runif(sum(is_censored), min = 0, max = 0.5)

t_lower <- runif(N, min = -2, max = 0)
t_upper <- runif(N, min = 0, max = 2)

is_observed <- t_lower <= x & x <= t_upper

obs <- trunc_obs(
  xmin = pmax(x_lower, t_lower)[is_observed],
  xmax = pmin(x_upper, t_upper)[is_observed],
  tmin = t_lower[is_observed],
  tmax = t_upper[is_observed]
)
```

Observations look like:

```{r}
obs[8L:12L, ]
```

The total number of observations is smaller than the base population of $1000$ due to truncation:
```{r}
nrow(obs)
```

The total number of censored observations is roughly $0.8 \cdot \mathtt{nrow(obs)}$.
```{r}
sum(is.na(obs$x))
```

In addition to the `trunc_obs()` constructor function, there are `as_trunc_obs()` for coercion, `truncate_obs()` for artifically changing truncation bounds, and `repdel_obs()` for computing randomly truncated reporting delay observations from general insurance claims data containing accident date, reporting delay and evaluation date information.
The latter takes inputs of the form $(T_\text{acc}, D, \tau)$ and returns the sample $(\mathtt{xmin} = \mathtt{xmax} = D, \mathtt{tmin} = 0, \mathtt{tmax} = \tau - T_{\text{acc}}, \mathtt{w} = 1)$ suitable for estimating the reporting delay distribution where a claim is only observed if it has been reported by the evaluation date, i.e., $T_{\text{acc}} + D \le \tau$.

## Definition of distribution families {#distributions}

Distribution families are implemented using the \pkg{R6} class system \citep{R6}.
They inherit from `Distribution` and feature a common interface to

* Manage fixed and free parameters of the underlying familiy
* Basic distribution functions for sampling, computation of density, cumulative density, hazard and quantile
* Additional functions supporting parameter estimation procedures such as computing support and discreteness
* Performance enhancements to speed up basic functions for repeated evaluation
* \pkg{tensorflow}-specific implementations to support (deep) distributional regression

A `Distribution` object represents a family $\mathcal{F}$ parameterized by a fixed-dimensional parameter space $\Theta$.
This implies that, e.g., for mixture distributions, the total number of component distributions and their families needs to be fixed.

\pkg{reservr} provides a set of basic distributions with fixed parameters as well as some transformations of distributions that take one or more underlying distributions.
At the time of writing, these are:

| Generator function &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Description                                                                                                                                                                            |
|---------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `dist_bdegp(n, m, u, epsilon)`                                                                                      | A Blended Dirac-Erlang-Generalized Pareto distribution, see Section&nbsp;\ref{dist-bdegp}                                                                                              |
| `dist_beta(shape1, shape2, ncp)`                                                                                    | A (non-central) Beta distribution                                                                                                                                                      |
| `dist_binomial(size, prob)`                                                                                         | A Binomial distribution                                                                                                                                                                |
| `dist_blended(dists, probs, breaks, bandwidths)`                                                                    | A Blended mixture distribution, see Section&nbsp;\ref{dist-blended}                                                                                                                    |
| `dist_dirac(point)`                                                                                                 | A Dirac distribution with full mass at `point`                                                                                                                                         |
| `dist_discrete(size, probs)`                                                                                        | A discrete distribution with support $\{1, \ldots, \text{size}\}$ and $P(X = k) = \text{probs}_k$                                                                                      |
| `dist_empirical(sample, positive, bw)`                                                                              | An empirical distribution derived from a sample, with density estimated using a kernel density estimate                                                                                |
| `dist_erlangmix(shapes, scale, probs)`                                                                              | An Erlang mixture distribution, i.e., a mixture of Erlangs with common scale parameter $f_Y(y) = \sum_{i=1}^k \text{probs}_i \cdot \mathrm{d}\Gamma(y; \text{shapes}_i, \text{scale})$ |
| `dist_exponential(rate)`                                                                                            | An Exponential distribution                                                                                                                                                            |
| `dist_gamma(shape, rate)`                                                                                           | A Gamma distribution                                                                                                                                                                   |
| `dist_genpareto(u, sigmau, xi)`                                                                                     | A Generalized Pareto Distribution                                                                                                                                                      |
| `dist_genpareto1(u, sigmau, xi)`                                                                                    | A Generalized Pareto Distribution with the tail index $\xi$ constrained to $(0, 1)$                                                                                                    |
| `dist_lognormal(meanlog, sdlog)`                                                                                    | A Log-Normaldistribution                                                                                                                                                               |
| `dist_mixture(dists, probs)`                                                                                        | A general Mixture distribution $f_Y(y) = \sum_{i=1}^k \text{probs}_i \cdot f_{X_i}(y)$                                                                                                 |
| `dist_negbinomial(size, mu)`                                                                                        | A negative Binomial distribution                                                                                                                                                       |
| `dist_normal(mean, sd)`                                                                                             | A Normal distribution                                                                                                                                                                  |
| `dist_pareto(shape, scale)`                                                                                         | A Pareto Type I distribution, i.e., a Generalized Pareto distribution with `u` = $0$                                                                                                   |
| `dist_poisson(lambda)`                                                                                              | A Poisson distribution                                                                                                                                                                 |
| `dist_translate(dist, offset, multiplier)`                                                                          | An affine transformation $Y = a \cdot X + b$ of another distribution                                                                                                                   |
| `dist_trunc(dist, min, max)`                                                                                        | A truncated distribution, $f_Y(y) = \tfrac{f_X(y)}{\mathrm{Pr}(\text{min} \le X \le \text{max})} \mathbf{1}(\text{min} \le y \le \text{max})$                                          |
| `dist_uniform(min, max)`                                                                                            | A uniform distribution                                                                                                                                                                 |
| `dist_weibull(shape, scale)`                                                                                        | A Weibull distribution                                                                                                                                                                 |

### Parameters

Parameters of distribution families can either be fixed to a set value, or free.
Free parameters (_placeholders_) are those that should be estimated from data whereas fixed parameters are held fixed.
Most distribution methods have an argument `with_params` to provide values for the free parameters and need fully specified parameters to work.
For example, generating samples from a distribution is only possible if it is fully parameterized using fixed parameters and the `with_params` argument of `Distribution$sample()`.

Distributions have a set of fields and methods related to managing parameters

* `default_params`, `get_params()` gets or sets the list of all parameters and their fixed values, `NULL` represents a free parameter. `get_params()` traverses component distribution parameters while `default_params` contains the `Distribution` objects for the respective parameters.
* `get_placeholders()` gets the list of free parameters with `NULL` as values.
* `param_bounds`, `get_param_bounds()` gets or sets the domain of each parameter as an `Interval` object. Setting a bound via the `param_bounds` active binding allows restricting the natural parameter space of a distribution. `get_param_bounds()` only returns the bounds of free parameters while `param_bounds` includes all parameters.
* `get_param_constraint()` returns `NULL` or a function that evaluates non-box constraints on the parameter set. The function must return a list with elements `constraints` and `jacobian` or just a vector of constraint values (that need to be equal to $0$ for valid parameters), optionally together with the jacobian of the constraint function. Used in `nloptr::slsqp(heq=)` for estimation. An example is that mixture distributions require the `probs` parameters to sum to $1$ in addition to the box constraint that each parameter is in $[0, 1]$.
* `get_components()` returns a list of component distributions for transformations or mixtures. The list is empty for basic families.

Here is an example for a normal family with fixed standard deviation $\sigma = 1$ and a mixture distribution with two components, one of which is specified as a normal distribution:

```{r}
dist <- dist_normal(sd = 1.0)
mix <- dist_mixture(dists = list(dist_normal(), NULL))

dist$default_params
mix$default_params
str(dist$get_placeholders())
str(mix$get_placeholders())
str(dist$param_bounds)
str(mix$param_bounds)
str(dist$get_param_bounds())
str(mix$get_param_bounds())
str(dist$get_param_constraints())
str(mix$get_param_constraints())
dist$get_components()
mix$get_components()
```

### Basic distribution functions

The basic distribution functions (density, probability, hazard, quantile, sample) are provided by each distribution with signatures.
In general, the argument `with_params` can be used to both specify missing parameters (placeholders) and to override fixed distribution parameters.
If the provided parameters are vectors of length greater than 1, they must conform to the input dimension (e.g. `x` for `density`).
In this case, the parameters are "vectorized" in the sense that the $i$th output element will be computed using the $i$th entry from the parameter list.

 * `density(x, log = FALSE, with_params = list())` compute (log-)density.
 * `probability(q, lower.tail = TRUE, log.p = FALSE, with_params = list()` compute (log-)cumulative density or (log-)survival.
 * `hazard(x, log = FALSE. with_params = list())` compute (log-)hazard.
 * `quantile(p, lower.tail = TRUE, log.p = FALSE, with_params = list())` compute upper or lower quantiles.
 * `sample(n, with_params = list())` generate a random sample (`with_params` can contain length `n` vectors in this case).

### Additional functions

In addition to the basic functions, there are several supporting functions useful for faster or more accurate estimation of parameters.
These functions are not necessarily implemented for all distribution classes, but will be automatically used by, e.g., `fit_dist()` if useful.

 * `export_functions(name, with_params = list())` exports `{d,p,q,r}<name>` functions adhering to the common R convention for distribution functions.
 * `get_type()` returns one of `"continuous"`, `"discrete"`, or `"mixed"` depending on whether the distribution has a density with respect to the Lebesgue measure, the counting measure, or their sum.
 * `is_continuous()` and `is_discrete()` testing for the particular type.
 * `has_capability(caps)` gives information on whether a specific implementation provides some or all of the features described.
   Possible capabilities are `"sample"`, `"density"`, `"probability"`, `"quantile"`, `"diff_density"`, `"diff_probability"`, `"tf_logdensity"`, `"tf_logprobability"`.
 * `require_capability(caps)` errors if the specified capabilities are not implemented for the distribution at hand.
 * `is_discrete_at(x, with_params = list())` returns a logical vector indicating whether the distribution has a point mass at `x`.
 * `is_in_support(x, with_params = list())` returns a logical vector indicating whether the distribution has any mass at `x`.

### Performance enhancements

When working with larger data or many calls to distribution functions, such as when performing a fit, it can be beneficial to just-in-time compile specialized functions that avoid overhead for dealing with the generic structure of distributions and their parametrization.
Distributions offer a set of "compiler" functions that return simplified, faster, versions of the basic distribution functions, or that avoid having to numerically compute gradients where possible.

 * `compile_density()` compiles a fast function with signature `(x, param_matrix, log = FALSE)` that will compute the density with fixed parameters hard-coded and taking the free parameters as a matrix with defined layout instead of a nested list.
 * `compile_probability()` compiles a fast replacement for `probability` with signature `(q, param_matrix, lower.tail = TRUE, log.p = FALSE)`.
 * `compile_probability_interval()` compiles a fast function with signature `(qmin, qmax, param_matrix, log.p = FALSE)` computing $P(X \in [\mathtt{qmin}, \mathtt{qmax}])$ or its logarithm efficiently. This expression is necessary for computing truncation probabilities.
 * `compile_sample()` compiles a fast replacement for `sample` with signature `(n, param_matrix)`.
 * `diff_density(x, log = FALSE, with_params = list())` computes the (log-)gradients of the density function with respect to free distribution parameters, useful for maximum likelihood optimization.
 * `diff_probability(q, lower.tail = TRUE, log.p = FALSE, with_params = list())` computes the (log-)gradients of the cumulative density function with respect to free distribution parameters

### \pkg{tensorflow} interface {short-title="tensorflow interface"}

Use of distributions from within \pkg{tensorflow} networks requires specialized implementations using the \pkg{tensorflow} APIs instead of regular \proglang{R} functions.
These are tailored to the needs of maximizing (conditional) likelihoods of weighted, censored and randomly truncated data.

 * `tf_compile_params(input, name_prefix = "")` creates \pkg{keras} layers that take an `input` layer and transform it into a valid parametrization of the distribution.
 * `tf_is_discrete_at()` returns a \pkg{tensorflow}-ready version of `is_discrete_at()`.
 * `tf_logdensity()` returns a \pkg{tensorflow}-ready version of  `compile_density()` with implied `log = TRUE`.
 * `tf_logprobability()` returns a \pkg{tensorflow}-ready version pf `compile_probability_interval()` with implied `log.p = TRUE`.
 * `tf_make_constants()` creates a list of constant tensors for all fixed distribution parameters.

### Blended distribution families {#dist-blended}

TODO describe blended distributions

### The Blended Dirac-Erlang-Generalized Pareto distribution family {#dist-bdegp}

TODO describe BDEGP family

## Methods of estimating distribution parameters {#fit-dist}

Sometimes, the conditional likelihood in \eqref{eq:cml-likelihood} can be directly maximized, yielding an estimate for $\theta$.
This is the default behavior in \pkg{reservr} if no specialized estimation routine for the provided family $\mathcal{F}_\theta$ is found.
Depending on whether there are box constraints, nonlinear constraints or no constraints on the parameter space $\Theta$, different implementations of nonlinear optimization algorithms from \pkg{nloptr} are employed.

In addition to the naive direct optimization approach, some families lend themselves to specialized estimation algorithms which usually show faster convergence due to making use of special structures in the parameter space $\Theta$.

At the time of writing there are specialized algorithms for three types of families:

1. Erlang mixtures (GEM-CMM algorithm from \citet{Gui2018}, with a fixed number of components $M$)
2. Blended distributions (Algorithm 2 in \citet{Rosenstock2022})
3. General mixture distributions (Algorithm 1 in \citet{Rosenstock2022})

An erlang mixture family is defined by their number of components, $k$, by

\[
  \Theta = \mathbb{R}_+ \times \mathbb{N}^k \times [0, 1]_k,
\]

where $[0, 1]_k := \{ (p_i)_{i = 1}^{k} \in [0, 1]^k | \sum_{i = 1}^{k} p_i = 1 \}$ is a $k$-dimensional mixture parameter.
And the density

\[
  f_\theta(x; \lambda, a_1, \ldots, a_k, p_1, \ldots, p_k) = \sum_{i = 1}^k p_i \mathrm{d}\Gamma(a_i, \lambda),
\]
where $\mathrm{d}\Gamma(a, \lambda)$ denotes the density of a Gamma distribution with shape $a$ and scale $\lambda$.

Blended distributions were defined in \citet{Rosenstock2022}.
They are, essentially, a smooth version of piecewise mixtures.

General mixture distributions are defined by its number of components, $k$, the component families $\mathcal{F}_1, \ldots, \mathcal{F}_k$, and their parameter spaces $\Theta_1, \ldots, \Theta_k$ by
\begin{align*}
  \Theta = \bigotimes_{i = 1}^k \Theta_i \times [0, 1]_k \\
  f_\theta(x; \theta_1, \ldots, \theta_k, p_1, \ldots, p_k) = \sum_{i = 1}^k p_i f_i(x; \theta_i)
\end{align*}

TODO describe ECME-Algorithms

## Distributional regression using \pkg{tensorflow} integration {short-title="distributional regression using tensorflow integration" #tensorflow}

# Conclusion {#conclusion}


[//]: # (old content:)

# Usage

This is a basic example which shows how to fit a normal distribution to randomly truncated and censored data.

\pkg{reservr} can be used to fit a normal distribution to the observations described in `obs` as follows.
First, the distribution family to be fitted is constructed.
It is possible to provide a-priori fixed values for some parameters as well, but we will not do this in this example.

```{r}
dist <- dist_normal()
```

Next, we can instruct \pkg{reservr} to estimate the parameters of `dist` from truncated observations `obs` using the `fit()` generic method.
This generic automatically selects an estimation algorithm depending on the distribution family that is provided.
In the case of a normal distribution, a generic \pkg{nloptr} based approach will be used.

```{r}
the_fit <- fit(dist, obs)
str(the_fit)
```

The fit results contain the estimated parameters, the result object of the call to \pkg{nloptr}, and the value of the conditional log-likelihood attained at the estimated parameters.
Using the function `plot_distributions()` we can also assess the quality of the fit.

```{r}
plot_distributions(
  true = dist,
  fitted = dist,
  empirical = dist_empirical(0.5 * (obs$xmin + obs$xmax)),
  .x = seq(-5, 5, length.out = 201),
  plots = "density",
  with_params = list(
    true = list(mean = 0.0, sd = 1.0),
    fitted = the_fit$params
  )
)
```

## Working with Distributions

Distributions are a set of classes available in \pkg{reservr} to specify distribution families of random variables.
A Distribution inherits from the R6 Class `Distribution` and provides all functionality necessary for working with a
specific family.

A Distribution can be defined by calling one of the constructor functions, prefixed by `dist_` in the package.
All constructors accept parameters of the family as arguments.
If these arguments are specified, the corresponding parameter is considered _fixed_ in the sense that it need not be
specified when computing something for the distribution and it will be assumed fixed when calling `fit()` on the
distribution instance.

### Sample

For example, an unspecified normal distribution can be created by calling `dist_normal()` without arguments.
This means the parameters `mean` and `sd` are considered _placeholders_.
If we want to, e.g., sample from `norm`, we must specify these placeholders in the `with_params` argument:

```{r}
set.seed(1L)
# Instantiate an unspecified normal distribution
norm <- dist_normal()
x <- norm$sample(n = 10L, with_params = list(mean = 3, sd = 1))

set.seed(1L)
norm2 <- dist_normal(sd = 1)
x2 <- norm2$sample(n = 10L, with_params = list(mean = 3))

# The same RVs are drawn because the distribution parameters
# and the seed are the same.
stopifnot(identical(x, x2))
```

### Density

The `density()` function computes the density of the distribution with respect to its natural measure.
Use `is_discrete_at()` to check if a point has discrete mass or lebesgue density.

```{r}
norm$density(x, with_params = list(mean = 3, sd = 1))
dnorm(x, mean = 3, sd = 1)
# Compute the log-density:
norm$density(x, log = TRUE, with_params = list(mean = 3, sd = 1))
norm$is_discrete_at(x, with_params = list(mean = 3, sd = 1))

# A discrete distribution with mass only at point = x[1].
dd <- dist_dirac(point = x[1])
dd$density(x)
dd$is_discrete_at(x)
```

`diff_density()` computes the gradient of the density with respect to each free parameter.
Setting `log = TRUE` computes the gradient of the log-density, i.e., the gradient of $\log f(x, params)$ instead.

```{r}
norm$diff_density(x, with_params = list(mean = 3, sd = 1))
```

### Probability

With `probability()`, the c.d.f., survival function, and their logarithms can be computed.
For discrete distributions, `dist$probability(x, lower.tail = TRUE)` returns $P(X \le x)$ and
`dist$probability(x, lower.tail = FALSE)` returns $P(X > x)$.

```{r}
norm$probability(x, with_params = list(mean = 3, sd = 1))
pnorm(x, mean = 3, sd = 1)

dd$probability(x)
dd$probability(x, lower.tail = FALSE, log.p = TRUE)
```

Gradients of the (log-)c.d.f. or survival function with respect to parameters can be computed using
`diff_probability()`.

```{r}
norm$diff_probability(x, with_params = list(mean = 3, sd = 1))
```

### Hazard

The hazard rate is defined by $h(x, \theta) = f(x, \theta) / S(x, \theta)$, i.e., the ratio of the density to the
survival function.

```{r}
norm$hazard(x, with_params = list(mean = 3, sd = 1))
norm$hazard(x, log = TRUE, with_params = list(mean = 3, sd = 1))
```

### Fitting

The `fit()` generic is defined for Distributions and will perform conditional maximum likelihood estimation, or a ECME-type algorithm depending on the Distribution family.
It accepts a weighted, censored and truncated sample of class `trunc_obs`, but can also accept uncensored,
untruncated observations without weight in the form of a numeric vector.
```{r}
# Fit with mean, sd free
fit1 <- fit(norm, x)
# Fit with mean free
fit2 <- fit(norm2, x)
# Fit with sd free
fit3 <- fit(dist_normal(mean = 3), x)

# Fitted parameters
str(fit1$params)
str(fit2$params)
str(fit3$params)

# log-Likelihoods can be computed on
AIC(fit1$logLik)
AIC(fit2$logLik)
AIC(fit3$logLik)

# Convergence checks
fit1$opt$message
fit2$opt$message
fit3$opt$message
```

### Fitting censored data

Estimation from censored data is done by constructing an appropriate `trunc_obs` object.

```{r}
params <- list(mean = 30, sd = 10)
x <- norm$sample(100L, with_params = params)
xl <- floor(x)
xr <- ceiling(x)

cens_fit <- fit(norm, trunc_obs(xmin = xl, xmax = xr))
str(cens_fit)
```

### Fitting truncated data

Similarly, randomly truncated data can be supplied.
There is a helper function `truncate_obs()` to artificially truncate observations.

```{r}
params <- list(mean = 30, sd = 10)
x <- norm$sample(100L, with_params = params)
tl <- runif(length(x), min = 0, max = 20)
tr <- runif(length(x), min = 0, max = 60) + tl

trunc_fit <- fit(norm, truncate_obs(x, tl, tr))
str(trunc_fit)

x_keep <- tl <= x & x <= tr
trunc_fit2 <- fit(norm, trunc_obs(x = x[x_keep], tmin = tl[x_keep], tmax = tr[x_keep]))
stopifnot(identical(trunc_fit, trunc_fit2))

attr(trunc_fit$logLik, "nobs")
```

### Plotting

Visualising different distributions, or parametrizations, e.g., fits, can be done with the function `plot_distributions()`.

```{r}
# Plot fitted densities
plot_distributions(
  true = norm,
  fit1 = norm,
  fit2 = norm2,
  fit3 = dist_normal(3),
  .x = seq(-2, 7, 0.01),
  with_params = list(
    true = list(mean = 3, sd = 1),
    fit1 = fit1$params,
    fit2 = fit2$params,
    fit3 = fit3$params
  ),
  plots = "density"
)

# Plot fitted densities, c.d.f.s and hazard rates
plot_distributions(
  true = norm,
  cens_fit = norm,
  trunc_fit = norm,
  .x = seq(0, 60, length.out = 101L),
  with_params = list(
    true = list(mean = 30, sd = 10),
    cens_fit = cens_fit$params,
    trunc_fit = trunc_fit$params
  )
)

# More complex distributions
plot_distributions(
  bdegp = dist_bdegp(2, 3, 10, 3),
  .x = c(seq(0, 12, length.out = 121), 1.5 - 1e-6),
  with_params = list(
    bdegp = list(
      dists = list(
        list(), list(), list(
          dists = list(
            list(
              dist = list(
                shapes = as.list(1:3),
                scale = 2.0,
                probs = list(0.2, 0.5, 0.3)
              )
            ),
            list(
              sigmau = 0.4,
              xi = 0.2
            )
          ),
          probs = list(0.7, 0.3)
        )
      ),
      probs = list(0.15, 0.1, 0.75)
    )
  )
)
```

## Working with \pkg{tensorflow} {short-title="Working with tensorflow" #working-with-tensorflow}

\pkg{reservr} also provides integration with \pkg{tensorflow} \citep{tensorflow} to allow training arbitrary neural networks using the negative conditional likelihood as a loss.
Resulting fits can be used to predict distribution parameters of the specified distribution given arbitrary input predictors.
The core of this integration is `tf_compile_model()`, which returns a specialized `reservr_keras_model` object with `fit()` and `predict()` methods.

```{r tensorflow-example, eval = FALSE}
library(ggplot2)
set.seed(1431L)
tensorflow::set_random_seed(1432L)

dataset <- tibble::tibble(
  x = runif(100, min = 10, max = 20),
  y = 2 * x + rnorm(100)
)

# Specify distributional assumption of OLS:
dist <- dist_normal(sd = 1.0) # OLS assumption: homoskedasticity

# Optional: Compute a global fit
global_fit <- fit(dist, dataset$y)

# Define a neural network
nnet_input <- keras::layer_input(shape = 1L, name = "x_input")
# in practice, this would be deeper
nnet_output <- nnet_input

optimizer <- keras::optimizer_adam(learning_rate = 0.1)

nnet <- tf_compile_model(
  inputs = list(nnet_input),
  intermediate_output = nnet_output,
  dist = dist,
  optimizer = optimizer,
  censoring = FALSE, # Turn off unnecessary features for this problem
  truncation = FALSE
)

nnet_fit <- fit(
  nnet,
  x = dataset$x,
  y = dataset$y,
  epochs = 100L,
  batch_size = 100L,
  shuffle = FALSE
)

plot(nnet_fit)

pred_params <- predict(nnet, data = list(keras::k_constant(dataset$x)))

lm_fit <- lm(y ~ x, data = dataset)

dataset$y_pred <- pred_params$mean
dataset$y_lm <- predict(lm_fit, newdata = dataset, type = "response")

ggplot(dataset, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = y_pred), color = "blue") +
  geom_line(aes(y = y_lm), linetype = 2L, color = "green")

coef_nnet <- rev(as.numeric(nnet$model$get_weights()))
coef_lm <- coef(lm_fit)

str(coef_nnet)
str(coef_lm)
```

## An actuarial application

The package also contains the function `prob_report()`, designed to compute the expected proportion of claims that are reported to an insurer at some calendar time interval $[\tau_0, \tau_1]$ given they occurred during calendar time $[t_0, t_1]$.
Some additional assumptions are necessary:

1. Claims during $[t_0, t_1]$ occur according to a Poisson Point Process $\xi$ with intensity $\lambda(t)$.
2. All occurred claims are independently marked with their reporting delay $D$, which is also independent of the occurrence time $T \in [t_0, t_1]$.

Then
\begin{align*}
  \mathbb E(\xi(\{(t, d) : t \in [t_0, t_1], t + d \in [\tau_0, \tau_1]\})) & = \int_{t_0}^{t_1} \lambda(t) P(\tau_0 \le t + D \le \tau_1) \,\mathrm{d}t \\
  & = \int_{t_0}^{t_1} \lambda(t) P_D([\tau_0 - t, \tau_1 - t]) \mathrm{d}t \\
  \mathbb E(\xi(\{(t, d) : t \in [t_0, t_1]\})) & = \int_{t_0}^{t_1} \lambda(t) \mathrm{d}t \\
  \mathtt{prob\_report}(t_0, t_1, \tau_0, \tau_1; \lambda) & = \frac{\int_{t_0}^{t_1} \lambda(t) P_D([\tau_0 - t, \tau_1 - t]) \mathrm{d}t}{\int_{t_0}^{t_1} \lambda(t) \mathrm{d}t},
\end{align*}

and for $\lambda \equiv \text{const}$, this further simplifies to
$$
  \mathtt{prob\_report}(t_0, t_1, \tau_0, \tau_1; \lambda) = \frac{1}{t_1 - t_0} \int_{t_0}^{t_1} P_D([\tau_0 - t, \tau_1 - t]) \mathrm{d}t.
$$

The \proglang{R} function `prob_report()` takes $(t_0, t_1, \tau_0, \tau_1)$ as a `trunc_obs` tibble with `xmin` and `xmax` taking the role of $t_0$ and $t_1$, and with `tmin` and `tmax` representing $\tau_0$ and $\tau_1$. $\lambda$ is passed as the argument `expo` which, if equal to `NULL`, assumes a homogeneous Poisson Point Process, i.e., $\lambda \equiv \text{const}$. For example, if $D \sim \mathrm{Exp}(r)$ and $\lambda(t) \equiv \text{const}$ is constant, the reporting probabilities for the first 5 development periods with $r = 1.0$ and $r = 0.5$ can be computed using the following \proglang{R} code.

```{r}
dist <- dist_exponential()
ints <- data.frame(
  xmin = 0,
  xmax = 1,
  tmin = rep(seq_len(5) - 1.0, times = 2L),
  tmax = rep(seq_len(5), times = 2L)
)
params <- list(rate = rep(c(1, 0.5), each = 5))

prob_report(dist, ints, with_params = params)
```

# Package architecture

This section describes the software architecture of the package.
A more task-focused introduction to the interface is provided in [Usage].
Note that the most up-to-date documentation for \pkg{reservr} can be found at https://ashesitr.github.io/reservr/index.html.

## Fitting

Every distribution supported by reservr is represented by an R6 class, inheriting from the R6 class `Distribution`.
Additionally, there are three S3 generics relevant to estimation of parameters for `Distribution`s:

 * `fit_dist_start()` to obtain suitable starting values for distribution fits, based on a `trunc_obs` observation object.
 * `fit_dist()` to perform estimation of _free_ parameters of the distribution.
 * `fit()`, delegating to `fit_dist()`.

The result of `fit_dist_start(dist, obs)` is a list of starting values compatible with `dist$get_placeholders()`.
Instead of passing a `trunc_obs` object to derive starting values from, a numeric vector of observations can also be used.

```{r}
dist <- dist_normal(sd = 1.0)
x <- dist$sample(100L, with_params = list(mean = 3.0))
str(fit_dist_start(dist, x))
```

A `fit_dist(dist, obs)` result is a list with elements

 * `params`: The final estimated parameters, compatible with `dist$get_placeholders()`.
 * `opt` (optional): The return value from `nloptr` if conditional ML was used to estimate the parameters.
 * `iter` (optional): The number of EM-Iterations performed if an EM-type algorithm was used to estimate the parameters.
 * `params_hist` (optional): The history of inner EM parameter estimates (only if fit with `trace = TRUE`).
 * `logLik`: An object of class 'logLik' containing the final log-likelihood and with `df` equal to `dist$get_dof()`.

## Parameters

A Distribution instance, created by the corresponding `dist_*()` function represents a family $\mathcal{F}$ of distributions.
Parameters of the underlying distribution can be held fixed (removing a degree of freedom from the family), or kept as a _placeholder_.

For example, `dist_normal()` represents the family of normal distributions with parameters $(\mu, \sigma) \in \mathbb{R} \times (0, \infty)$.
`dist_normal(mean = 0.0)` represents the family of centered normal distributions with mean $0$ and parameter $\sigma \in (0, \infty)$.

Distribution parameters are stored as named lists, with the sentinel value `NULL` representing a free parameter.
The instance method `get_params()` returns a list of all parameters, both fixed and free, and `get_placeholders()` returns a list structure containing only the free parameters.
`default_params` is an active binding with the same structure as `get_params()` but the possibility for overwriting values.

```{r}
dist <- dist_normal(mean = 0.0)
str(dist$get_params())
str(dist$default_params)
str(dist$get_placeholders())
```

The free parameters have associated domains, which can be queried using `get_param_bounds()`, which returns a list with the same structure as `get_placeholders()`, but with leaves not equal to `NULL` but instead the real or integer `Interval` bounds for the parameter.
If there are additional constraints, such as for mixture distributions, where the probability weights parameters must add up to $1$, these constraints are available via `get_param_constraints()`.
This function returns a constraint function which returns the value and the Jacobian of the constraints with respect to the parameters, and whose value must be zero for admissible parameters.
All distributions also provide a `get_components()` function returning a list of component distributions (or an empty list).

```{r}
str(dist$get_param_bounds())
str(dist$get_param_constraints())

# Mixture of two components
mixd <- dist_mixture(
  dists = list(
    dist_normal(mean = 0.0),
    dist_normal(sd = 1.0)
  )
)
str(mixd$get_placeholders())
str(mixd$get_param_constraints())
str(mixd$get_components())
```

The degrees of freedom of a family, defined as the dimension of the space of admissible parameters is used for returning information criteria of distribution fits and is returned by the instance method `get_dof()`.
```{r}
c(
  dist_normal()$get_dof(),
  dist$get_dof(),
  mixd$get_dof()
)
```
